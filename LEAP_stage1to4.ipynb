{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting plinkio\n",
      "  Downloading plinkio-0.9.6.tar.gz\n",
      "Building wheels for collected packages: plinkio\n",
      "  Running setup.py bdist_wheel for plinkio ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n",
      "\u001b[?25h  Stored in directory: /home/jovyan/.cache/pip/wheels/6f/e2/58/0f2d910f7aa9154227bf313074794f255534879d9f0b66e863\n",
      "Successfully built plinkio\n",
      "Installing collected packages: plinkio\n",
      "Successfully installed plinkio-0.9.6\n"
     ]
    }
   ],
   "source": [
    "!pip install plinkio\n",
    "##Load data:\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plinkio import plinkfile\n",
    "import time\n",
    "import sklearn.covariance as Cov\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from statsmodels.discrete.discrete_model import Probit\n",
    "#from scipy.linalg.blas import dsyrk \n",
    "    #--can't find a way to get this working. Perhaps blas routines are missing.\n",
    "    \n",
    "data_path = '/home/jovyan/work/LEAP/leap/regression/dataset1'\n",
    "os.chdir(data_path)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "author: gene burinskiy\n",
    "\n",
    "Goal: \n",
    "Finding a set of individuals who are related to other individuals in the study. \n",
    "LEAP employs a greedy algorithm to find a small subset of such individuals, \n",
    "such that after their exclusion, there are no related individuals in the study. \n",
    "These individuals are excluded from the analysis in stages 3 and 4 below, \n",
    "but after fitting a model in stage 4, their liabilities are estimated along with \n",
    "other indviduals. All individuals are considered in the GWAS stage (stage 5).\n",
    "\n",
    "source for algorithm and methods. References are cited in functions. \n",
    "https://github.com/omerwe/LEAP/blob/master/leap/regression/Leap_example.ipynb\n",
    "\n",
    "I, in some cases, heavily modify or write new code to perform the tasks in the\n",
    "Goal. \n",
    "\n",
    "NOTES: this is an exploration file and includes code that would otherwise be redundant. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of locuses 10499\n",
      "# of chromosomes in data: [ 1  2  3  4  5  6  7  8  9 10]\n",
      "Number of individuals in data: 1000\n"
     ]
    }
   ],
   "source": [
    "##Load data:\n",
    "bed = plinkfile.open(\"dataset1\")\n",
    "\n",
    "loci = bed.get_loci()\n",
    "print(\"Length of locuses\", len(loci))\n",
    "chromosomes = np.unique([x.chromosome for x in loci])\n",
    "print(\"# of chromosomes in data:\",chromosomes)\n",
    "\n",
    "samples = bed.get_samples()\n",
    "print(\"Number of individuals in data:\", len(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data type: int16\n",
      "Size of bed matrix:   20mb\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, person1 to person1000\n",
      "Columns: 10499 entries, (1, csnp18) to (10, snp10483)\n",
      "dtypes: int16(10499)\n",
      "memory usage: 20.0+ MB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>chromosome</th>\n",
       "      <th colspan=\"5\" halign=\"left\">1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>snp</th>\n",
       "      <th>csnp18</th>\n",
       "      <th>csnp35</th>\n",
       "      <th>csnp59</th>\n",
       "      <th>csnp78</th>\n",
       "      <th>csnp85</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person3</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person4</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "chromosome      1                            \n",
       "snp        csnp18 csnp35 csnp59 csnp78 csnp85\n",
       "person1         2      1      1      2      1\n",
       "person2         1      0      2      2      2\n",
       "person3         0      2      2      2      2\n",
       "person4         2      1      2      2      1\n",
       "person5         0      1      2      1      2"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Place data into a dataframe:\n",
    "mat = np.zeros((len(loci),len(samples)), dtype='int16') #1/4 of the taken up space by using int16\n",
    "\n",
    "##don't know a faster method of extracting the data from the bed file.\n",
    "i=0\n",
    "for row in bed:\n",
    "    mat[i,:] = np.array([snp for snp in row])\n",
    "    i+=1\n",
    "    \n",
    "#this matrix is equivalent to transposed bed.val\n",
    "print(\"Data type:\", mat.dtype)\n",
    "print(\"Size of bed matrix: %4.0fmb\\n\" %(mat.nbytes/(1024**2)))\n",
    "\n",
    "#create a multi-indexed column space\n",
    "tuples = [(x.chromosome,x.name) for x in loci]\n",
    "ml_index = pd.MultiIndex.from_tuples(tuples, names = ['chromosome', 'snp'])\n",
    "\n",
    "df = pd.DataFrame(mat.transpose(), columns=ml_index, index = [x.iid for x in bed.get_samples()]) \n",
    "df.info()\n",
    "df.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 1000 entries, person1 to person1000\n",
      "Columns: 10499 entries, (1, csnp18) to (10, snp10483)\n",
      "dtypes: float64(10499)\n",
      "memory usage: 80.1+ MB\n",
      "\n",
      "Covariance shape: (1000, 1000)\n",
      "Covariance memory usage in mb: 7.62939453125\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.36816713,  0.00128849, -0.00865588, -0.00119474,  0.0038927 ],\n",
       "       [ 0.00128849,  0.35826201,  0.0033948 ,  0.00228287,  0.00136917],\n",
       "       [-0.00865588,  0.0033948 ,  0.36285412,  0.00443604, -0.00057367],\n",
       "       [-0.00119474,  0.00228287,  0.00443604,  0.36310693,  0.00183888],\n",
       "       [ 0.0038927 ,  0.00136917, -0.00057367,  0.00183888,  0.37099567]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##compute covariance matrix between individuals, remove those who are too close to each other.\n",
    "#they LEAP code uses dsyrk which halves the computational time. Alas, we can't use it y\n",
    "\n",
    "df = df.astype('float64')-df.astype('float64').mean() \n",
    "df.info()\n",
    "\n",
    "dot_cov = np.dot(df, df.transpose())/(df.shape[1]-1) #having difficulties with scipy's linalg module\n",
    "#note that the above takes more than half the time of np.cov\n",
    "print(\"\\nCovariance shape:\" , dot_cov.shape)\n",
    "print(\"Covariance memory usage in mb:\", dot_cov.nbytes/(1024**2))\n",
    "dot_cov[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##trying to use a shrinkage estimator for the covariance matrix. Inspired by the computeCovar\n",
    "##function in https://github.com/omerwe/LEAP/blob/master/leap/leapUtils.py\n",
    "oa_fit = Cov.OAS().fit(df.transpose())\n",
    "lw_fit = Cov.LedoitWolf().fit(df.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under Oracle shrinkage:\n",
      "shape of y: (45,)\n",
      "[ 20  37  47  62  66  76  82  89 104 112 128 142 151 156 160 161 163 164\n",
      " 165 172 173 189 194 196 197 199 205 209 210 211 213 214 217 221 222 232\n",
      " 233 234 241 242 244 246 355 439 499]\n"
     ]
    }
   ],
   "source": [
    "cutoff = .05\n",
    "oa_cov = oa_fit.covariance_\n",
    "bool_arr =  np.tril(oa_cov, k=-1)>cutoff\n",
    "y_idx,_ = np.where(bool_arr)\n",
    "print(\"Under Oracle shrinkage:\")\n",
    "print(\"shape of y:\",np.unique( y_idx).shape)\n",
    "print(np.unique(y_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under numpy cov:\n",
      "shape of y: (47,)\n",
      "[ 20  37  47  62  66  76  82  89 104 112 128 142 151 156 160 161 162 163\n",
      " 164 165 172 173 187 189 194 196 197 199 205 209 210 211 213 214 217 221\n",
      " 222 232 233 234 241 242 244 246 355 439 499]\n"
     ]
    }
   ],
   "source": [
    "np_cov = np.cov(df, ddof=0)\n",
    "bool_arr =  np.tril(np_cov, k=-1)>cutoff\n",
    "y_idx,_ = np.where(bool_arr)\n",
    "print(\"Under numpy cov:\")\n",
    "print(\"shape of y:\", np.unique(y_idx).shape)\n",
    "print(np.unique(y_idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Under Ledoit-Wolf estimate\n",
      "shape of y: (37,)\n",
      "[ 20  37  66  76  82  89 104 112 128 142 156 161 163 164 172 173 189 194\n",
      " 197 199 205 209 210 211 213 214 217 221 222 232 233 234 242 244 246 246\n",
      " 355 439]\n"
     ]
    }
   ],
   "source": [
    "lw_cov = lw_fit.covariance_\n",
    "bool_arr =  np.tril(lw_cov, k=-1)>cutoff\n",
    "y_idx,_ = np.where(bool_arr)\n",
    "print(\"Under Ledoit-Wolf estimate\")\n",
    "print(\"shape of y:\", np.unique(y_idx).shape)\n",
    "print(y_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np_s, np_U = np.linalg.eigh(np_cov, 'L')\n",
    "oa_s,oa_U = np.linalg.eigh(oa_cov, 'L')\n",
    "lw_s, lw_U = np.linalg.eigh(lw_cov, 'L')\n",
    "dot_s, dot_U = np.linalg.eigh(dot_cov, 'L')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.05943703e-12   1.30036478e-01   1.31650207e-01]\n",
      "[ 0.57159555  0.57757169  2.35200119]\n",
      "[ 0.1773261   0.24439076  0.24522302]\n",
      "[ -1.49183313e-06   1.30050436e-01   1.31666958e-01]\n"
     ]
    }
   ],
   "source": [
    "print(np_s[:3])\n",
    "print(oa_s[-3:])\n",
    "print(lw_s[:3])\n",
    "print(dot_s[:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of y: (56,)\n",
      "\n",
      "removing 56 individuals\n",
      "Original num of ppl: 1000\n",
      "num of kept ppl: 953\n"
     ]
    }
   ],
   "source": [
    "cutoff = .05\n",
    "bool_arr =  np.tril(dot_cov, k=-1)>cutoff\n",
    "y_idx,_ = np.where(bool_arr)\n",
    "print(\"shape of y:\", y_idx.shape)\n",
    "print(\"\\nremoving %d individuals\" %y_idx.shape[0])\n",
    "\n",
    "#note, they marked 54 so we marked more peeps, we effectively remove 56 rows. Something doesn't line up.\n",
    "indxToKeep = set(range(dot_cov.shape[0]))\n",
    "print(\"Original num of ppl:\", len(indxToKeep))\n",
    "\n",
    "[indxToKeep.remove(i) for i in np.unique(y_idx)]\n",
    "keepArr = np.array(list(indxToKeep))\n",
    "print(\"num of kept ppl:\", keepArr.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.203\n",
      "22.2029\n"
     ]
    }
   ],
   "source": [
    "#exploring different ways to exclude individuals found above.\n",
    "cov_m = np.ma.array(cov,mask=False)\n",
    "cov_m.mask[y_idx,:] = True\n",
    "cov_m.mask[:,y_idx] = True\n",
    "\n",
    "print(cov_m.sum())\n",
    "\n",
    "cov_c = np.delete(np.delete(cov, y_idx, axis=0), y_idx, axis=1)\n",
    "print(cov_c.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fam</th>\n",
       "      <th>pheno</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>person1</th>\n",
       "      <td>FAM1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person2</th>\n",
       "      <td>FAM1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person3</th>\n",
       "      <td>FAM1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person4</th>\n",
       "      <td>FAM1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>person5</th>\n",
       "      <td>FAM1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          fam  pheno\n",
       "person              \n",
       "person1  FAM1      0\n",
       "person2  FAM1      0\n",
       "person3  FAM1      0\n",
       "person4  FAM1      0\n",
       "person5  FAM1      0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Our calc_h2 function for Step 3\n",
    "#uses the calc_h2.calc_h2 functions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import stats\n",
    "\n",
    "#read in phenofile:\n",
    "phenos = pd.read_csv(\"dataset1.phe\", sep=' ', header=None, engine='c')\n",
    "phenos.columns = ['fam', 'person', 'pheno']\n",
    "phenos.set_index(keys = 'person', inplace=True)\n",
    "phenos.iloc[:5,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcLiabThresholds_3xx(U,s, keepArr, phe, numRemovePCs=10, prevalence = .001, covar=None): \n",
    "    \"\"\"\n",
    "    INPUTS:\n",
    "        1. U - left eigenvectors of covariance matrix (ie kinship matrix)\n",
    "        2. S - eigenvalues of covariance matrix (ie kinship matrix)\n",
    "        3. keepArr - np.array of indexes that exclude highly related individuals\n",
    "        4. phe - np.array of phenotypes (binary only)\n",
    "        5. covar - god knows. specified in author functions but remains undefined. \n",
    "    OUTPUT:\n",
    "        1. probs - probability estimates from a regularized logistic regression\n",
    "        2. threshold - no idea what this is, I assume they're estimated liabilities?\n",
    "    NOTES:\n",
    "        original code can be found on:\n",
    "        https://github.com/omerwe/LEAP/blob/master/leap/calc_h2.py\n",
    "    \"\"\"\n",
    "    ##------------------------------------ CalcLiabThreshold -----------------------------------\n",
    "    ##probs, thresholds = calcLiabThreholds(U, S, keepArr, phe, numRemovePCs, prevalence, covar) \n",
    "\n",
    "    #This is equivalent to an SVD decomposition; note their covar parameter is defaulted to None\n",
    "    G = U[:, -numRemovePCs:] * np.sqrt(s[-numRemovePCs:])\n",
    "\n",
    "    #perform a regularized logistic regression. I trust their parameter settings.\n",
    "    Logreg = LogisticRegression(penalty='l2', C=500000, fit_intercept=True)\n",
    "    Logreg.fit(G[keepArr, :], phe.iloc[keepArr])\n",
    "\n",
    "    #Compute individual thresholds\n",
    "    probs = Logreg.predict_proba(G)[:,1]\n",
    "    \n",
    "    #Compute thresholds\n",
    "    prev = prevalence\n",
    "    P = np.sum(phe==1) / float(phe.shape[0])\n",
    "    #K = prev --why, why in the (insert explicative) hell do they do this?\n",
    "    Ki = prev*(1-prev) / (P*(1-prev)) * probs / (1 + prev*(1-prev) / (P*(1-prev))*probs - probs)\n",
    "    thresholds = stats.norm(0,1).isf(Ki)\n",
    "    thresholds[Ki>=1.] = -999999999\n",
    "    thresholds[Ki<=0.] = 999999999\n",
    "    \n",
    "    return([probs, thresholds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calcH2Binary(XXT_o, phe_o, probs_o, thresholds_o, keepArr_o, prev, h2coeff):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        1. XXT - covariance matrix (kinship matrix) * number of snps\n",
    "        2. phe - np.array of phenotypes. In our case, they're binary.\n",
    "        3. probs - np.array of probabilities\n",
    "        4. thresholds - np.array of something (I believe they're estimated liabilities)\n",
    "        5. keepArr - np.array of indexes that exclude highly related individuals.\n",
    "        6. prev - prevalence\n",
    "        7. h2coeff - no idea. they set it to 1.0 for synthetic data. .875 otherwise\n",
    "    NOTES:\n",
    "        Many items have been removed for sake of more compact code. Namely, the actions if\n",
    "        thresholds is None. \n",
    "        Original code can be found on:\n",
    "        https://github.com/omerwe/LEAP/blob/master/leap/calc_h2.py\n",
    "    \"\"\"\n",
    "    K = prev\n",
    "    P = np.sum(phe_o>0) / float(phe_o.shape[0])\n",
    "    \n",
    "    #index out individuals we do not want. In order to avoid reassining variables,\n",
    "    #I assign the input objects to new objects which are views.\n",
    "    XXT = XXT_o[np.ix_(keepArr, keepArr)]\n",
    "    phe = phe_o[keepArr]\n",
    "\n",
    "    probs = probs_o[keepArr]\n",
    "    thresholds = thresholds_o[keepArr]\n",
    "    \n",
    "    Ki = K*(1-P) / (P*(1-K)) * probs / (1 + K*(1-P) / (P*(1-K))*probs - probs)\n",
    "    phit = stats.norm(0,1).pdf(thresholds)\n",
    "    probsInvOuter = np.outer(probs*(1-probs), probs*(1-probs))\n",
    "    y = np.outer(phe-probs, phe-probs) / np.sqrt(probsInvOuter)\t\n",
    "    sumProbs = np.tile(np.column_stack(probs).T, (1,probs.shape[0])) + np.tile(probs, (probs.shape[0], 1))\n",
    "    Atag0 = np.outer(phit, phit) * (1 - (sumProbs)*(P-K)/(P*(1-K)) + np.outer(probs, probs)*(((P-K)/(P*(1-K)))**2)) / np.sqrt(probsInvOuter)\n",
    "    B0 = np.outer(Ki + (1-Ki)*(K*(1-P))/(P*(1-K)), Ki + (1-Ki)*(K*(1-P))/(P*(1-K)))\n",
    "    x = (Atag0 / B0 * h2coeff) * XXT\n",
    "\n",
    "    y = y[np.triu_indices(y.shape[0], 1)]\n",
    "    x = x[np.triu_indices(x.shape[0], 1)]\n",
    "\n",
    "    slope, intercept, rValue, pValue, stdErr = stats.linregress(x,y)\n",
    "    \n",
    "    return slope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.linear_model\n",
    "import scipy.optimize as opt\n",
    "\n",
    "def evalProbitReg(beta, X, cases, controls, thresholds, invRegParam, normPDF, h2):\n",
    "    \"\"\"\n",
    "    NOTES: not much to do here as everything is in numpy. \n",
    "    \"\"\"\n",
    "    XBeta = np.ravel(X.dot(beta)) - thresholds\n",
    "    phiXBeta = normPDF.pdf(XBeta)\n",
    "    PhiXBeta = normPDF.cdf(XBeta)\n",
    "\n",
    "    logLik = np.sum(np.log(PhiXBeta[cases])) + np.sum(np.log(1-PhiXBeta[controls]))\t\n",
    "    w = np.zeros(X.shape[0])\n",
    "    w[cases] = -phiXBeta[cases] / PhiXBeta[cases]\n",
    "    w[controls] = phiXBeta[controls] / (1-PhiXBeta[controls])\n",
    "    grad = X.T.dot(w)\n",
    "\n",
    "    #regularize\n",
    "    logLik -= 0.5*invRegParam * beta.dot(beta)\t#regularization\t\n",
    "    grad += invRegParam * beta\n",
    "    return (-logLik, grad)\n",
    "\n",
    "def probitRegHessian(beta, X, cases, controls, thresholds, invRegParam, normPDF, h2):\n",
    "    \"\"\"\n",
    "    NOTES: not much to do here as everything is in numpy. Though, I precalculated\n",
    "    PhiXBeta and then subset that because it was originally done for each subset. It is, trivially,\n",
    "    faster to precompute the element-wise squaring and then subset. \n",
    "    \"\"\"\n",
    "    XBeta = np.ravel(X.dot(beta)) - thresholds\n",
    "    phiXBeta = normPDF.pdf(XBeta)\n",
    "    PhiXBeta = normPDF.cdf(XBeta)\n",
    "\n",
    "    XbetaScaled = XBeta #/(1-h2)\n",
    "\n",
    "    #PhiXBeta2 = np.square(PhiXBeta)\n",
    "    R = np.zeros(X.shape[0])\n",
    "    R[cases] = (XbetaScaled[cases]*PhiXBeta[cases] + phiXBeta[cases]) / PhiXBeta[cases]**2\n",
    "    R[controls] = (-XbetaScaled[controls]*(1-PhiXBeta[controls]) + phiXBeta[controls]) / (1 - PhiXBeta[controls])**2\n",
    "\n",
    "    R *= phiXBeta\n",
    "    H = (X.T * R).dot(X)\n",
    "    H += invRegParam\n",
    "    return H\n",
    "\n",
    "\n",
    "def probitRegression(X, y, thresholds, numSNPs, numFixedFeatures, h2, useHess, maxFixedIters, epsilon, nofail):\n",
    "    \"\"\"\n",
    "    NOTE: If I had more time, I would probably use PyMC3 for this ... eventually. For now, just removed superfluous\n",
    "    parts.\n",
    "    0. print statement parantheses added. \n",
    "    1. Num of Fixed effects = 0 => delete fixed effect estimation code. \n",
    "    \"\"\"\n",
    "    regParam = h2 /  float(numSNPs)\t\n",
    "    Linreg = sklearn.linear_model.Ridge(alpha=1.0/(2*regParam), fit_intercept=False, normalize=False, solver='lsqr')\n",
    "    Linreg.fit(X, y)\n",
    "    initBeta = Linreg.coef_\n",
    "    np.random.seed(1234)\n",
    "\n",
    "    normPDF = stats.norm(0, np.sqrt(1-h2))\n",
    "    invRegParam = 1.0/regParam\n",
    "    controls = (y==0)\n",
    "    cases = (y==1)\n",
    "    funcToSolve = evalProbitReg\n",
    "    hess =(probitRegHessian if useHess else None)\n",
    "    jac= True\n",
    "    method = 'Newton-CG'\n",
    "    args = (X, cases, controls, thresholds, invRegParam, normPDF, h2)\n",
    "    print('Beginning Probit regression...')\n",
    "    t0 = time.time()\n",
    "    optObj = opt.minimize(funcToSolve, x0=initBeta, args=args, jac=jac, method=method, hess=hess)\n",
    "    print('Done in', '%0.2f'%(time.time()-t0), 'seconds')\n",
    "    if (not optObj.success):\n",
    "        print('Optimization status:', optObj.status)\n",
    "        print(optObj.message)\n",
    "        if (nofail == 0): raise Exception('Probit regression failed with message: ' + optObj.message)\n",
    "    beta = optObj.x\n",
    "\n",
    "    return beta\n",
    "\n",
    "def probit_3xx(df, phe, h2, prev, U,s, keepArr, thresholds=None,  covar=None,  nofail=0, outFile = None,\n",
    "    numSkipTopPCs=10, mineig=1e-3, hess=1, recenter=1, maxFixedIters=1e2, epsilon=1e-3, treatFixedAsRandom=False):\n",
    "    \"\"\"\n",
    "    INPUT:\n",
    "        1. df - pandas data frame of normalized snp values. df excludes current chromosome.\n",
    "        2. ph - pandas data frame of phenotypes\n",
    "        4. h2 - np.array of calculated something\n",
    "        5. prev - prevalence\n",
    "        6. U,S - left eigenvectors and eigenvalues.\n",
    "        7. thresholds - calculated thresholds. \n",
    "    Modifications:\n",
    "        1. No longer read in the bed, phenotype file, \n",
    "        2. no longer set binary phenotype cases.\n",
    "        3. get U and s directly, not from eigen dictionary\n",
    "        5. removed covar statement.\n",
    "        6. commented out saving of file as the current approach seems slow and requires interaction with the disk.\n",
    "            there are faster ways of saving these things BUT no time to fix that now until later. Also, just for\n",
    "            clarity, rewrote the code to adhere to modern Python practises. Finally, that section does not\n",
    "            seem to pertain to the output. \n",
    "        7. They use \"structure\" instead of dictionary -> C world. \n",
    "        8. Instead of a dictionary, I return a Pandas DataFrame ... because we <3 Pandas. Also, header was set to\n",
    "            None, so we remove it as output. Finally, we return directly, don't save it to LiabStructure or \n",
    "            liab_df in our case.\n",
    "        9. Replaced a few probitRegression inputs to adhere to our data structures.\n",
    "    Default parameters set from the argparse section in the original code. Original code can be found\n",
    "    in:\n",
    "    https://github.com/omerwe/LEAP/blob/master/leap/probit.py\n",
    "    \"\"\"\n",
    "    ncols = df.shape[1]\n",
    "\n",
    "    #run probit regression\n",
    "    t = stats.norm(0,1).isf(prev)\n",
    "    if (thresholds is not None): t = thresholds\n",
    "\n",
    "    S = np.sqrt(s*ncols)\n",
    "    goodS = (S>mineig)\n",
    "    if (numSkipTopPCs > 0): goodS[-numSkipTopPCs:] = False\n",
    "    if (np.sum(~goodS) > 0): print('\\t\\tRemoving', np.sum(~goodS), 'PCs with low variance')\n",
    "    G = U[:, goodS]*S[goodS]\n",
    "    \n",
    "    #Recenter G\tto only consider the unrelated individuals\n",
    "    if recenter: G -= np.mean(G[keepArr, :], axis=0)\n",
    "    else: G -= np.mean(G, axis=0)\n",
    "    \n",
    "    #Run Probit regression\n",
    "    numFixedFeatures = 0\n",
    "    probitThresh = (t if thresholds is None else t[keepArr])\n",
    "    \n",
    "    #I believe bed.sid.shape is the sid_count. In our data, that is sid_count = df.shape[1] \n",
    "    beta = probitRegression(G[keepArr, :], phe[keepArr], probitThresh, ncols, numFixedFeatures, h2, hess, maxFixedIters, epsilon, nofail)\n",
    "    \n",
    "    #predict probabilies\n",
    "    meanLiab = G.dot(beta)\n",
    "    liab = meanLiab.copy()\n",
    "    indsToFlip = np.logical_or( np.logical_and((liab <= t), (phe>0.5)), np.logical_and((liab > t),(phe<0.5)))\n",
    "    liab[indsToFlip] = stats.norm(0,1).isf(prev)\n",
    "    \n",
    "    #Return phenotype dictionary with liabilities\n",
    "    return pd.DataFrame( {'vals':liab,'person':df.index})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on chromosome: 1\n",
      "\th2 values: 0.438676"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:13: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:15: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:16: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:38: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:39: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:13: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\t\tRemoving 10 PCs with low variance\n",
      "Beginning Probit regression...\n",
      "Done in 0.08 seconds\n",
      "Optimization status: 2\n",
      "Desired error not necessarily achieved due to precision loss.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:15: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n",
      "/opt/conda/lib/python3.4/site-packages/ipykernel/__main__.py:16: FutureWarning: in the future, boolean array-likes will be handled as a boolean array index\n"
     ]
    },
    {
     "ename": "Exception",
     "evalue": "Probit regression failed with message: Desired error not necessarily achieved due to precision loss.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-1df6085c05c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\th2 values: %f\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[0mh2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 43\u001b[1;33m     \u001b[0mliabs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobit_3xx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphenos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpheno\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprevalence\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mU\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepArr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\t Took %.2f seconds\"\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mt0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-34a29416da85>\u001b[0m in \u001b[0;36mprobit_3xx\u001b[1;34m(df, phe, h2, prev, U, s, keepArr, thresholds, covar, nofail, outFile, numSkipTopPCs, mineig, hess, recenter, maxFixedIters, epsilon, treatFixedAsRandom)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[1;31m#I believe bed.sid.shape is the sid_count. In our data, that is sid_count = df.shape[1]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m     \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprobitRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mG\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeepArr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mphe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkeepArr\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprobitThresh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mncols\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumFixedFeatures\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhess\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxFixedIters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnofail\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m     \u001b[1;31m#predict probabilies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-34a29416da85>\u001b[0m in \u001b[0;36mprobitRegression\u001b[1;34m(X, y, thresholds, numSNPs, numFixedFeatures, h2, useHess, maxFixedIters, epsilon, nofail)\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Optimization status:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnofail\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Probit regression failed with message: '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0moptObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m     \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptObj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: Probit regression failed with message: Desired error not necessarily achieved due to precision loss."
     ]
    }
   ],
   "source": [
    "#with multi-index, we index by using the number of the chromosome. \n",
    "#This avoids copying of data -> we use views on the data. Immeasurably more efficient\n",
    "\n",
    "prevalence = .001\n",
    "numRemovePCs = 10\n",
    "#chrom = chromosomes[2]\n",
    "for chrom in chromosomes[:3]:\n",
    "    print(\"Working on chromosome: %s\" %chrom)\n",
    "\n",
    "    exclude_chrom = set(chromosomes)\n",
    "    exclude_chrom.remove(chrom) #set all chromosomes except current\n",
    "    exclude_chrom = list(exclude_chrom)\n",
    "\n",
    "    t0 = time.time()\n",
    "    #Note that the original code puts cov, s, U into a dictionary called \"eigen\"\n",
    "    #They do not actually perform an SVD decomposition. Instead, they compute\n",
    "    #the covariance matrix, decompose that and use an equivalence relation between\n",
    "    #SVD and the decomposition of the covariance matrix. \n",
    "\n",
    "    #XXT = np.dot(df[exclude_chrom], df[exclude_chrom].transpose())\n",
    "    #s,U = np.linalg.eigh(XXT, 'L') #would use scipy except -again- can't get it to load.\n",
    "\n",
    "    XXT = Cov.OAS().fit(df[exclude_chrom].transpose()).covariance_\n",
    "    s,U = np.linalg.eigh(XXT, 'L')\n",
    "\n",
    "    #calc_h2 function\n",
    "    if numRemovePCs>0:\n",
    "        t_XXT = XXT -  (U[:,-numRemovePCs:]*s[-numRemovePCs:]).dot(U[:,-numRemovePCs:].transpose())\n",
    "\n",
    "    pheUnique = np.unique(phenos.pheno)\n",
    "    isCaseControl = pheUnique.shape[0] == 2 #trivial condition for us\n",
    "\n",
    "    if ~np.all(pheUnique == np.array([0,1])):\n",
    "        pheMean = phenos.pheno.mean()\n",
    "        phenos.pheno[phenos.pheno <= pheMean] = 0\n",
    "        phenos.pheno[phenos.pheno> pheMean] = 1\n",
    "\n",
    "    probs, thresholds= calcLiabThresholds_3xx(U,s, keepArr, phenos.pheno)\n",
    "    #h2coeff = .875 but set to 1.0 for synthetic data. \n",
    "    h2 = calcH2Binary(t_XXT, phenos.pheno, probs, thresholds, keepArr, prev=prevalence, h2coeff=1.0)\n",
    "    print(\"\\th2 values: %f\" %h2)\n",
    "    \n",
    "    liabs = probit_3xx(df, phenos.pheno, h2, prevalence, U,s, keepArr)\n",
    "\n",
    "    print(\"\\t Took %.2f seconds\" %(time.time()-t0))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 154 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.dot(df, df.transpose())/df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 loop, best of 3: 354 ms per loop\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "np.cov(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
